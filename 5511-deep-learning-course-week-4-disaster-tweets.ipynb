{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### General information\nThis mini-project is part of the Week 4 assignment for the University of Colorado Boulder’s Data Science Deep Learning course.","metadata":{}},{"cell_type":"markdown","source":"### Step One \n\nBrief description of the problem and data (5 pts)\n\nThis competition challenges participants to build a machine learning model that classifies whether a tweet is about a real disaster or not. Using a dataset of 10,000 hand-labeled tweets, participants will apply Natural Language Processing (NLP) techniques to analyze textual content and distinguish between disaster-related tweets and non-disaster tweets. The competition is beginner-friendly, with a manageable dataset size and tools like Kaggle Notebooks for easy, free access to a coding environment. This is an excellent opportunity to learn and apply NLP skills to a real-world problem, with community support available on Kaggle’s Discord server.\n\n","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\nThis starter notebook is provided by the Keras team.</center>\n\n## Keras NLP starter guide here: https://keras.io/guides/keras_nlp/getting_started/\n\nIn this competition, the challenge is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.\nA dataset of 10,000 tweets that were hand classified is available. \n\n__This starter notebook uses the [DistilBERT](https://arxiv.org/abs/1910.01108) pretrained model from KerasNLP.__\n\n\n**BERT** stands for **Bidirectional Encoder Representations from Transformers**. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\nThe BERT family of models uses the **Transformer encoder architecture** to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\n**DistilBERT model** is a distilled form of the **BERT** model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n\n\n\n![BERT Architecture](https://www.cse.chalmers.se/~richajo/nlp2019/l5/bert_class.png)\n\n\n\nIn this notebook, you will:\n\n- Load the Disaster Tweets\n- Explore the dataset\n- Preprocess the data\n- Load a DistilBERT model from Keras NLP\n- Train your own model, fine-tuning BERT\n- Generate the submission file\n","metadata":{}},{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp --upgrade\n\n# This sample uses Keras Core, the multi-backend version of Keras.\n# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-26T01:46:38.723136Z","iopub.execute_input":"2024-11-26T01:46:38.723637Z","iopub.status.idle":"2024-11-26T01:47:02.406093Z","shell.execute_reply.started":"2024-11-26T01:46:38.723595Z","shell.execute_reply":"2024-11-26T01:47:02.404543Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting keras-core\n  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.23.5)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-core) (13.4.2)\nCollecting namex (from keras-core)\n  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core) (3.9.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.1.8)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.15.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\nInstalling collected packages: namex, keras-core\nSuccessfully installed keras-core-0.1.7 namex-0.0.8\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import tensorflow.keras.layers as layers  # For TensorFlow-based Keras\n#import keras_nlp                         # For NLP-specific utilities\nimport keras               # For Keras-Core","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:47:02.408983Z","iopub.execute_input":"2024-11-26T01:47:02.409345Z","iopub.status.idle":"2024-11-26T01:47:11.715546Z","shell.execute_reply.started":"2024-11-26T01:47:02.409310Z","shell.execute_reply":"2024-11-26T01:47:11.714504Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install keras-nlp==0.6.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:47:11.716880Z","iopub.execute_input":"2024-11-26T01:47:11.717595Z","iopub.status.idle":"2024-11-26T01:47:22.378215Z","shell.execute_reply.started":"2024-11-26T01:47:11.717563Z","shell.execute_reply":"2024-11-26T01:47:22.376974Z"}},"outputs":[{"name":"stdout","text":"Collecting keras-nlp==0.6.1\n  Downloading keras_nlp-0.6.1-py3-none-any.whl (573 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.5/573.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: keras-core in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (0.1.7)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (2023.6.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (13.4.2)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.6.1) (2.12.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp==0.6.1) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp==0.6.1) (3.9.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core->keras-nlp==0.6.1) (0.1.8)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-nlp==0.6.1) (3.0.9)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp==0.6.1) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp==0.6.1) (2.15.1)\nRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-nlp==0.6.1) (0.12.0)\nRequirement already satisfied: tensorflow<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-nlp==0.6.1) (2.12.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp==0.6.1) (0.1.2)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.51.3)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.3.6)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp==0.6.1) (3.2.2)\nInstalling collected packages: keras-nlp\n  Attempting uninstall: keras-nlp\n    Found existing installation: keras-nlp 0.17.0\n    Uninstalling keras-nlp-0.17.0:\n      Successfully uninstalled keras-nlp-0.17.0\nSuccessfully installed keras-nlp-0.6.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:22.380618Z","iopub.execute_input":"2024-11-26T01:47:22.380981Z","iopub.status.idle":"2024-11-26T01:47:23.948869Z","shell.execute_reply.started":"2024-11-26T01:47:22.380942Z","shell.execute_reply":"2024-11-26T01:47:23.947762Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using TensorFlow backend\nTensorFlow version: 2.12.0\nKerasNLP version: 0.6.1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Load the Disaster Tweets\nLet's have a look at the train and test dataset.\n\nThey contain:\n- id\n- keyword: A keyword from that tweet (although this may be blank!)\n- location: The location the tweet was sent from (may also be blank)\n- text: The text of a tweet\n- target: 1 if the tweet is a real disaster or 0 if not","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:23.952474Z","iopub.execute_input":"2024-11-26T01:47:23.952931Z","iopub.status.idle":"2024-11-26T01:47:24.046173Z","shell.execute_reply.started":"2024-11-26T01:47:23.952896Z","shell.execute_reply":"2024-11-26T01:47:24.045004Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.047456Z","iopub.execute_input":"2024-11-26T01:47:24.047874Z","iopub.status.idle":"2024-11-26T01:47:24.066805Z","shell.execute_reply.started":"2024-11-26T01:47:24.047836Z","shell.execute_reply":"2024-11-26T01:47:24.065522Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.068718Z","iopub.execute_input":"2024-11-26T01:47:24.069083Z","iopub.status.idle":"2024-11-26T01:47:24.079765Z","shell.execute_reply.started":"2024-11-26T01:47:24.069055Z","shell.execute_reply":"2024-11-26T01:47:24.078459Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"!pip install sweetviz\nimport sweetviz as sv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:49:07.533981Z","iopub.execute_input":"2024-11-26T01:49:07.534381Z","iopub.status.idle":"2024-11-26T01:49:18.325765Z","shell.execute_reply.started":"2024-11-26T01:49:07.534348Z","shell.execute_reply":"2024-11-26T01:49:18.324625Z"}},"outputs":[{"name":"stdout","text":"Collecting sweetviz\n  Downloading sweetviz-2.3.1-py3-none-any.whl (15.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (1.5.3)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (1.23.5)\nRequirement already satisfied: matplotlib>=3.1.3 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (3.7.1)\nRequirement already satisfied: tqdm>=4.43.0 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (4.65.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (1.11.1)\nRequirement already satisfied: jinja2>=2.11.1 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (3.1.2)\nRequirement already satisfied: importlib-resources>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from sweetviz) (5.12.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.1->sweetviz) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\nInstalling collected packages: sweetviz\nSuccessfully installed sweetviz-2.3.1\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Generate the Sweetviz analysis report\nreport_train = sv.analyze(df_train)\nreport_test = sv.analyze(df_test)\n# Display the report in a browser\nreport_train.show_html(\"sweetviz_train.html\")\nreport_test.show_html(\"sweetviz_test.html\")\n\n\n# Display the report in the notebook\nfrom IPython.display import IFrame\nIFrame(src=\"sweetviz_train.html\", width=1000, height=600)\nIFrame(src=\"sweetviz_test.html\", width=1000, height=600)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:53:36.583106Z","iopub.execute_input":"2024-11-26T01:53:36.583470Z","iopub.status.idle":"2024-11-26T01:53:39.848168Z","shell.execute_reply.started":"2024-11-26T01:53:36.583442Z","shell.execute_reply":"2024-11-26T01:53:39.846766Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                             |          | [  0%]   00:00 -> (? left)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74bc0cc8baa499e815e630b88139b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                             |          | [  0%]   00:00 -> (? left)","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b4bcc1cf7249fdb848c56515a9f973"}},"metadata":{}},{"name":"stdout","text":"Report sweetviz_train.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\nReport sweetviz_test.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<IPython.lib.display.IFrame at 0x7b497d2ad4b0>","text/html":"\n        <iframe\n            width=\"1000\"\n            height=\"600\"\n            src=\"sweetviz_test.html\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Explore the dataset","metadata":{}},{"cell_type":"code","source":"df_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n\nprint(\"Train Length Stat\")\nprint(df_train[\"length\"].describe())\nprint()\n\nprint(\"Test Length Stat\")\nprint(df_test[\"length\"].describe())","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.473929Z","iopub.status.idle":"2024-11-26T01:47:24.474304Z","shell.execute_reply.started":"2024-11-26T01:47:24.474133Z","shell.execute_reply":"2024-11-26T01:47:24.474150Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you want to know more information about the data, you can grab useful information [here](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n\nNote that all the tweets are in english.","metadata":{}},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.476858Z","iopub.status.idle":"2024-11-26T01:47:24.477230Z","shell.execute_reply.started":"2024-11-26T01:47:24.477063Z","shell.execute_reply":"2024-11-26T01:47:24.477081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_train[\"text\"]\ny = df_train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = df_test[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.478390Z","iopub.status.idle":"2024-11-26T01:47:24.478756Z","shell.execute_reply.started":"2024-11-26T01:47:24.478558Z","shell.execute_reply":"2024-11-26T01:47:24.478573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load a DistilBERT model from Keras NLP\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n\nThe BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n\nWe will choose DistilBERT model.that learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n\nIt has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n\nSpecifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT.","metadata":{}},{"cell_type":"code","source":"# Load a DistilBERT model.\npreset= \"distil_bert_base_en_uncased\"\n\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n                                                                   sequence_length=160,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)\n\nclassifier.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.479725Z","iopub.status.idle":"2024-11-26T01:47:24.480074Z","shell.execute_reply.started":"2024-11-26T01:47:24.479911Z","shell.execute_reply":"2024-11-26T01:47:24.479928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train your own model, fine-tuning BERT","metadata":{}},{"cell_type":"code","source":"# Compile\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n    optimizer=keras.optimizers.Adam(1e-5),\n    metrics= [\"accuracy\"]  \n)\n\n# Fit\nhistory = classifier.fit(x=X_train,\n                         y=y_train,\n                         batch_size=BATCH_SIZE,\n                         epochs=EPOCHS, \n                         validation_data=(X_val, y_val)\n                        )","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.481480Z","iopub.status.idle":"2024-11-26T01:47:24.481852Z","shell.execute_reply.started":"2024-11-26T01:47:24.481646Z","shell.execute_reply":"2024-11-26T01:47:24.481662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        np.argmax(y_pred, axis=1),\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.483884Z","iopub.status.idle":"2024-11-26T01:47:24.484389Z","shell.execute_reply.started":"2024-11-26T01:47:24.484136Z","shell.execute_reply":"2024-11-26T01:47:24.484162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_train = classifier.predict(X_train)\n\ndisplayConfusionMatrix(y_train, y_pred_train, \"Training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:47:24.486083Z","iopub.status.idle":"2024-11-26T01:47:24.486428Z","shell.execute_reply.started":"2024-11-26T01:47:24.486267Z","shell.execute_reply":"2024-11-26T01:47:24.486284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_val = classifier.predict(X_val)\n\ndisplayConfusionMatrix(y_val, y_pred_val, \"Validation\")","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.488093Z","iopub.status.idle":"2024-11-26T01:47:24.488441Z","shell.execute_reply.started":"2024-11-26T01:47:24.488277Z","shell.execute_reply":"2024-11-26T01:47:24.488294Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate the submission file \n\nFor each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nThe `submission.csv` file uses the following format:\n`id,target`","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:47:24.490554Z","iopub.status.idle":"2024-11-26T01:47:24.490972Z","shell.execute_reply.started":"2024-11-26T01:47:24.490800Z","shell.execute_reply":"2024-11-26T01:47:24.490818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T01:47:24.492778Z","iopub.status.idle":"2024-11-26T01:47:24.493288Z","shell.execute_reply.started":"2024-11-26T01:47:24.493040Z","shell.execute_reply":"2024-11-26T01:47:24.493066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.describe()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.496360Z","iopub.status.idle":"2024-11-26T01:47:24.496767Z","shell.execute_reply.started":"2024-11-26T01:47:24.496565Z","shell.execute_reply":"2024-11-26T01:47:24.496583Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-26T01:47:24.498285Z","iopub.status.idle":"2024-11-26T01:47:24.498606Z","shell.execute_reply.started":"2024-11-26T01:47:24.498455Z","shell.execute_reply":"2024-11-26T01:47:24.498470Z"},"trusted":true},"outputs":[],"execution_count":null}]}